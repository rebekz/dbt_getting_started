# dbt Learning Repository

A comprehensive learning repository for mastering dbt (Data Build Tool) with practical examples, tutorials, and end-to-end data pipeline implementations.

## ğŸ¯ Overview

This repository demonstrates professional data transformation workflows using dbt with DuckDB, covering everything from basic concepts to advanced analytics and machine learning feature engineering.

## ğŸ›  Tech Stack

- âœ… **dbt** - Data transformation framework
- âœ… **DuckDB** - Embedded analytical database
- âœ… **Python 3.12** - Runtime environment
- âœ… **uv** - Fast Python package manager

## ğŸ“š Learning Materials

### 1. Basic Tutorial
**File**: `dbt_tutorial.md`
- âœ… Introduction to dbt concepts
- âœ… Setting up your first project
- âœ… Basic model creation
- âœ… Testing and documentation

### 2. E-commerce Pipeline Example
**File**: `ecommerce_pipeline_example.md`
- âœ… Practical e-commerce use case
- âœ… Data modeling patterns
- âœ… Dimensional modeling
- âœ… Business logic implementation

### 3. E-commerce TDD Approach
**File**: `ecommerce_pipeline_tdd.md`
- âœ… Test-Driven Development with dbt
- âœ… Data quality frameworks
- âœ… Testing strategies
- âœ… CI/CD integration

### 4. **End-to-End Analytics Workflow** â­
**File**: `ecommerce_analytics_end_to_end.md`

Comprehensive tutorial demonstrating the complete data analytics lifecycle:

#### ğŸ“Š Modules Covered

1. **Data Wrangling**
   - Hierarchical indexing patterns
   - Data cleaning and standardization
   - Combining datasets (merge, concat, join)
   - Reshaping and pivoting data

2. **Data Aggregation**
   - Split-Apply-Combine methodology
   - RFM (Recency, Frequency, Monetary) analysis
   - Pivot tables and cross-tabulation
   - Customer behavior metrics

3. **Exploratory Data Analysis (EDA)**
   - Distribution analysis
   - Correlation analysis
   - Customer segmentation visualization
   - Product performance analysis

4. **Predictive Modeling Preparation**
   - Feature engineering for ML
   - Customer churn prediction features
   - Customer Lifetime Value (CLV) modeling
   - Training data preparation

#### ğŸ“ What You'll Build

- âœ… **Staging Layer**: Clean, standardized data from raw sources
- âœ… **Intermediate Models**: RFM analysis, customer orders aggregation
- âœ… **Data Marts**:
  - Customer metrics with CLV predictions
  - Customer segmentation (Champions, At Risk, etc.)
  - Product performance analytics
- âœ… **ML Features**: Ready-to-use features for:
  - Churn prediction models
  - CLV forecasting
  - Customer behavior analysis

#### ğŸ’¼ Business Use Cases

1. **Churn Prevention Campaigns**
   - Identify high-value at-risk customers
   - Targeted win-back strategies

2. **Marketing Budget Allocation**
   - CLV-based budget distribution
   - Segment-specific campaign planning

3. **Product Recommendations**
   - Customer-product affinity matrices
   - Collaborative filtering data prep

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Python 3.12
uv python install 3.12
```

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd dbt_get_started

# Install dependencies
uv sync

# Navigate to the project
cd my_first_project

# Load seed data
dbt seed

# Run all models
dbt run

# Run tests
dbt test

# Generate and view documentation
dbt docs generate
dbt docs serve
```

## ğŸ§ª Seed Data Options

You can load the provided sample CSVs in two ways:

1. Configure dbt to include the root-level `data_seeds/` directory:

```yaml
# my_first_project/dbt_project.yml
seed-paths: ["seeds", "../data_seeds"]
```

2. Or copy the CSVs into the project's default seed folder:

```bash
cp -R data_seeds my_first_project/seeds/raw
```

Then run:

```bash
cd my_first_project
dbt seed
```

## ğŸ“ Project Structure

```
dbt_get_started/
â”œâ”€â”€ data_seeds/                 # Sample CSVs for seeding
â”‚   â”œâ”€â”€ raw_customers.csv
â”‚   â”œâ”€â”€ raw_orders.csv
â”‚   â”œâ”€â”€ raw_order_items.csv
â”‚   â””â”€â”€ raw_products.csv
â”œâ”€â”€ my_first_project/              # Main dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/               # Raw data cleaning & standardization
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_order_items.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_products.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/          # Business logic & aggregations
â”‚   â”‚   â”‚   â”œâ”€â”€ int_customer_orders.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ int_customer_rfm.sql
â”‚   â”‚   â”‚   â””â”€â”€ int_product_metrics.sql
â”‚   â”‚   â”œâ”€â”€ marts/                 # Analytics-ready tables
â”‚   â”‚   â”‚   â”œâ”€â”€ fct_customer_metrics.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customer_segments.sql
â”‚   â”‚   â”‚   â””â”€â”€ fct_product_performance.sql
â”‚   â”‚   â””â”€â”€ ml/                    # ML feature engineering
â”‚   â”‚       â””â”€â”€ ml_customer_features.sql
â”‚   â”œâ”€â”€ seeds/                     # Default seed folder (configure to include ../data_seeds)
â”‚   â”‚   â””â”€â”€ raw/
â”‚   â”œâ”€â”€ tests/                     # Custom data quality tests
â”‚   â”œâ”€â”€ dbt_project.yml           # Project configuration
â”‚   â””â”€â”€ my_dbt.duckdb             # DuckDB database
â”œâ”€â”€ dbt_tutorial.md               # Basic tutorial
â”œâ”€â”€ ecommerce_pipeline_example.md # E-commerce pipeline guide
â”œâ”€â”€ ecommerce_pipeline_tdd.md     # TDD approach guide
â”œâ”€â”€ ecommerce_analytics_end_to_end.md  # â­ Complete analytics workflow
â”œâ”€â”€ pyproject.toml                # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸ” Key Concepts Demonstrated
  - unique
  - not_null
  - accepted_values
  - relationships
```

### Custom Tests
```sql
# tests/assert_positive_clv.sql
select *
from {{ ref('fct_customer_metrics') }}
where predicted_clv_12m < 0
```

### Running Tests
```bash
# All tests
dbt test

# Specific model
dbt test --select fct_customer_metrics

# By tag
dbt test --select tag:customer
```

## ğŸ“Š Example Queries

### Customer Segmentation
```sql
select
    customer_segment,
    count(*) as customers,
    avg(monetary_value) as avg_value,
    avg(predicted_clv_12m) as avg_clv
from {{ ref('dim_customer_segments') }}
group by 1
order by avg_clv desc;
```

### Churn Risk Analysis
```sql
select
    churn_risk,
    customer_segment,
    count(*) as at_risk_customers,
    sum(monetary_value) as revenue_at_risk
from {{ ref('fct_customer_metrics') }}
where churn_risk in ('High', 'Medium')
group by 1, 2;
```

### Product Performance
```sql
select
    performance_tier,
    category,
    count(*) as product_count,
    sum(revenue) as total_revenue,
    sum(total_profit) as total_profit
from {{ ref('fct_product_performance') }}
group by 1, 2;
```

## ğŸ¯ Best Practices

1. **Modularity**: Keep models focused and reusable
2. **Documentation**: Document all models in schema.yml
3. **Testing**: Test at every layer (staging â†’ marts)
4. **Naming Conventions**:
   - `stg_` for staging models
   - `int_` for intermediate models
   - `fct_` for fact tables
   - `dim_` for dimension tables
   - `ml_` for ML features
5. **Materialization**:
   - Views for staging (fast, always fresh)
   - Tables for marts (performance)
   - Incremental for large datasets

## ğŸ”§ Common Commands

```bash
# Development workflow
dbt compile                    # Check SQL syntax
dbt run --select model_name    # Run specific model
dbt run --select +model_name   # Run model + upstream
dbt run --select model_name+   # Run model + downstream

# By tags
dbt run --select tag:staging
dbt run --select tag:ml

# Testing
dbt test
dbt test --select model_name

# Documentation
dbt docs generate
dbt docs serve --port 8080

# Full refresh
dbt run --full-refresh

# Production
dbt run --target prod
```

## ğŸš¢ Deployment Considerations

### For Production

1. **Orchestration**: Schedule with Airflow/Dagster/Prefect
2. **CI/CD**: Automate testing on PR
3. **Monitoring**: Set up data quality alerts
4. **Incremental Models**: Use for large datasets
5. **Performance**: Optimize materializations

### Example Airflow DAG
```python
from airflow import DAG
from airflow.operators.bash import BashOperator

dag = DAG('dbt_ecommerce_analytics', schedule_interval='@daily')

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='cd /path/to/project && dbt run',
    dag=dag
)

dbt_test = BashOperator(
    task_id='dbt_test',
    bash_command='cd /path/to/project && dbt test',
    dag=dag
)

dbt_run >> dbt_test
```

## ğŸ“ˆ Performance Tuning

- âœ… Use `{{ ref() }}` for model dependencies
- âœ… Leverage incremental models for large tables
- âœ… Optimize with appropriate materializations
- âœ… Use CTEs for readability and performance
- âœ… Index columns used in joins (database-specific)

## ğŸ¤ Contributing

Feel free to contribute by:
- âœ… Adding new examples
- âœ… Improving documentation
- âœ… Reporting issues
- âœ… Suggesting best practices

## ğŸ“ License

This is a learning repository for educational purposes.

## ğŸ”— Resources

- âœ… [dbt Documentation](https://docs.getdbt.com/)
- âœ… [dbt Discourse Community](https://discourse.getdbt.com/)
- âœ… [DuckDB Documentation](https://duckdb.org/docs/)
- âœ… [Analytics Engineering Guide](https://www.getdbt.com/analytics-engineering/)

## ğŸ’¡ What's Next?

After completing these tutorials, you'll be ready to:
- âœ… Build production data pipelines
- âœ… Implement data quality frameworks
- âœ… Create analytics-ready data models
- âœ… Prepare features for ML models
- âœ… Apply analytics engineering best practices

Happy learning! ğŸ‰
